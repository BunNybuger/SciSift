{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is to combine all json files you produced with SciSift into a CSV and view it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where JSON files are located\n",
    "json_directory = 'outputs'  # Adjust this path to your directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def is_json_file(filename):\n",
    "    \"\"\"Check if the file has a JSON extension (case insensitive).\"\"\"\n",
    "    return filename.lower().endswith(\".json\")\n",
    "\n",
    "data_list = []\n",
    "\n",
    "# Get a list of all JSON files in the directory\n",
    "json_files = [f for f in os.listdir(json_directory) if is_json_file(f)]\n",
    "\n",
    "# If there are no JSON files, exit\n",
    "if not json_files:\n",
    "    print(\"No JSON files found.\")\n",
    "    exit()\n",
    "\n",
    "# Read the first JSON file and set ordered_keys\n",
    "with open(os.path.join(json_directory, json_files[0]), 'r') as json_file:\n",
    "    first_data = json.load(json_file)\n",
    "    first_data['Filename'] = json_files[0]\n",
    "    ordered_keys = list(first_data.keys())\n",
    "    data_list.append(first_data)\n",
    "\n",
    "# Iterate through the remaining JSON files in the directory\n",
    "for filename in json_files[1:]:\n",
    "    with open(os.path.join(json_directory, filename), 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        data['Filename'] = filename\n",
    "        data_list.append(data)\n",
    "            \n",
    "        # If a key isn't in ordered_keys yet, append it\n",
    "        for key in data.keys():\n",
    "            if key not in ordered_keys:\n",
    "                ordered_keys.append(key)\n",
    "\n",
    "# Write the collected data to a CSV file\n",
    "csv_path = os.path.join(json_directory,'combined_data.csv')\n",
    "with open(csv_path, 'w', newline='') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=ordered_keys)\n",
    "    writer.writeheader()\n",
    "    for row in data_list:\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "# Read the CSV file and Display it\n",
    "df = pd.read_csv(csv_path)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install plotly==5.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "cells": {
          "align": "left",
          "fill": {
           "color": "lavender"
          },
          "values": [
           [
            "Learning accurate personal protective equipment detection from virtual worlds",
            "A Safety System based on Bluetooth Low Energy (BLE) to prevent the misuse of Personal Protection Equipment (PPE) in construction",
            "Automatic Construction Hazard Identification Integrating On-Site Scene Graphs with Information Extraction in Outfield Test",
            "Context-aware safety assessment system for far-field monitoring",
            "An embedded toolset for human activity monitoring in critical environments",
            "Generic compliance of industrial PPE by using deep learning techniques"
           ],
           [
            "(Di Benedetto et al., 2021)",
            "(Gómez-de-Gabriel et al., 2022)",
            "Liu, X., Jing, X., Zhu, Q., Du, W., & Wang, X. (2023). Automatic Construction Hazard Identification Integrating On-Site Scene Graphs with Information Extraction in Outfield Test. Buildings, 13(2), 377. https://doi.org/10.3390/buildings13020377",
            "(Chern et al., 2023)",
            "(Di Benedetto et al., 2022)",
            "(Vukicevic et al., 2022)"
           ],
           [
            2021,
            2022,
            2023,
            2023,
            2022,
            2022
           ],
           [
            "10.1007/s11042-020-09597-9",
            "https://doi.org/10.1016/j.ssci.2022.105995",
            "https://doi.org/10.3390/buildings13020377",
            "https://doi.org/10.1016/j.autcon.2023.104779",
            "https://doi.org/10.1016/j.eswa.2022.117125",
            "https://doi.org/10.1016/j.ssci.2021.105646"
           ],
           [
            "The paper proposes using photo-realistic synthetic images generated from a game engine to train deep learning models for detecting personal protective equipment (PPE). The models are then adapted to real-world images using a small set of real images. The approach is shown to be effective for PPE detection applications where limited real training data is available.",
            "The paper proposes a system to monitor the use of personal protective equipment (PPE) like earmuffs in construction sites. It uses multiple Bluetooth Low Energy (BLE) beacons on the PPE and a receiver on the tool to estimate distance between worker and tool. If distance is unsafe or PPE is not worn properly, the system can turn off tool power to prevent injury.",
            "The paper proposes a framework to automatically identify construction hazards by integrating on-site scene graph generation and BERT-based information extraction from safety regulations text. The information extraction model extracts key textual relational triples from regulations. The scene graph model detects visual relations between objects in images. By matching the textual and visual relational triples, the framework can conduct automatic safety checking and hazard identification.",
            "The paper proposes a context-aware safety monitoring system to assess workers' safety compliance in far-field construction site monitoring. It uses object detection, semantic segmentation and depth estimation models to distinguish workers at height vs on ground. This allows applying different PPE rules based on working context to reduce false alarms. Two approaches are tested using depth estimation and extended segmentation. The extended segmentation approach performed better in experiments.",
            "The paper presents a modular embedded system for monitoring human activities in critical environments. The system uses computer vision and AI to perform tasks like estimating crowd density, measuring social distance, and detecting personal protective equipment. The modules can be combined and configured based on needs. Experiments validate the system's effectiveness for safety monitoring.",
            "The paper proposes a 4-step procedure to automate compliance of personal protective equipment (PPE) in industrial settings using deep learning. It detects workers using pose estimation, defines regions of interest (ROI) for different PPE types, and classifies the ROIs as compliant or not. The approach is evaluated on 18 PPE types protecting different body parts, achieving 95% accuracy with MobileNetV2. It demonstrates improved performance and flexibility compared to prior work."
           ],
           [
            "Personal protective equipment like helmets, welding masks, ear protection, high visibility vests",
            "Use of PPE (earmuff), Distance between worker and tool",
            "The paper detects interactions between construction workers, personal protective equipment (PPE), and other objects in images of construction sites.",
            "Workers, PPE (hardhats, harnesses, straps, hooks), working context (height vs ground)",
            "The system detects pedestrians, measures interpersonal distances, estimates crowd density, and detects personal protective equipment like helmets, high-visibility vests, and face masks.",
            "Compliance of 18 different types of personal protective equipment (PPE)."
           ],
           [
            "{'PPE': ['Helmet', 'Welding Mask', 'Ear Protection', 'High-Visibility Vest'], 'Actions': [], 'Body posture': []}",
            "{'PPE': 'Earmuff', 'Actions': '', 'Body posture': ''}",
            "{'PPE': ['hard hats', 'eye protection', 'hand protection', 'face protection'], 'Actions': ['wearing', 'holding', 'standing', 'performing operations'], 'Body posture': []}",
            "{'PPE': ['hardhats', 'harnesses', 'straps', 'hooks'], 'Actions': [], 'Body posture': []}",
            "{'PPE': ['Helmets', 'High-visibility vests', 'Face masks'], 'Actions': [], 'Body posture': []}",
            "{'PPE': ['Face mask', 'Respiratory mask', 'Earmuffs', 'Welding mask', 'Face shields', 'Safety glasses', 'Hardhat', 'Head cover', 'Yellow vests', 'Visibility tracks', 'Exam gloves', 'Industry gloves', 'Sandals', 'Industry shoes', 'Boots', 'Feet covers', 'Protective cloth'], 'Body parts': ['Respiratory system', 'Hearing system', 'Face', 'Eyes', 'Head', 'Upper body', 'Hands', 'Feet', 'Whole body']}"
           ],
           [
            "YOLO v3 and Faster R-CNN deep learning models trained on synthetic images from Grand Theft Auto V game engine and adapted to real images.",
            "Multiple BLE beacons placed orthogonally on PPE, RSSI signals received by tool, Extended Kalman Filter and Bayesian filtering to estimate distance",
            "The paper uses a BERT-based model for information extraction from text and a CNN-based model for scene graph generation from images. The BERT model extracts key entities and relations from safety regulation text. The scene graph model uses Mask R-CNN for object detection and a separate CNN branch for relation detection.",
            "Object detection (YOLOv5), semantic segmentation (FPN), and depth estimation (pre-trained model by Alhashim and Wonka 2018) are used. The models are trained on the YUD-COSAv2 dataset collected from real construction sites.",
            "The system uses deep learning models like Faster R-CNN for pedestrian and PPE detection. It also uses a density estimation model based on CSRNet for crowd density estimation.",
            "Uses HigherHRNet pose estimator to detect workers and define regions of interest (ROI) for different PPE types. Classifies each ROI with deep learning classifiers (MobileNetV2, DenseNet, ResNet etc.) to determine PPE compliance."
           ],
           [
            "#vision",
            "#vision",
            "#vision",
            "#vision",
            "#vision",
            "#vision"
           ],
           [
            "['Grand Theft Auto V game engine', 'RAGE plugin']",
            "Microcontroller, Light sensor",
            "['fastText for word embeddings', 'LabelImg and PySimpleGUI for image annotation']",
            "['OpenCV', 'PyTorch']",
            "['Faster R-CNN', 'CSRNet', 'DeepSort tracker']",
            "['HigherHRNet pose estimator', 'MobileNetV2', 'DenseNet', 'ResNet', 'PyTorch']"
           ],
           [
            "Mean average precision (mAP)",
            "Compared estimated distance to ground truth measurements",
            "For information extraction: precision, recall, F1 score. For scene graph generation: average precision, recall@K.",
            "For object detection: mAP@0.5. For semantic segmentation: mIoU. For safety classification: Precision, Recall, F1-score.",
            "Mean average precision (mAP) for detection tasks. Mean absolute error (MAE), root mean squared error (RMSE) and structural similarity index (SSIM) for density estimation.",
            "Accuracy, Precision, Recall, F1 Score"
           ],
           [
            "76.1% mAP with YOLOv3, 77.1% mAP with Faster R-CNN after adaptation",
            "Expected error around 20 cm",
            "79.3% F1 score for information extraction. 50.7% Recall@20 for scene graph generation.",
            "mAP of 85.3% for YOLOv5. mIoU of 75.74% for FPN. F1-score of 95.07% for safety classification using extended segmentation.",
            "Pedestrian detection mAP: 0.836. PPE detection mAP: 0.606. Density estimation MAE: 92.28, RMSE: 365.4, SSIM: 0.79.",
            "95% overall accuracy with MobileNetV2"
           ],
           [
            "Shows effectiveness of using synthetic training data from game engines for training deep learning models when real training data is limited. Adaptation to real images with small real dataset is effective.",
            "Low cost system to monitor PPE use and worker-tool distance. Can actively turn off tool power instead of just warning. Robust to noise due to multiple orthogonal BLE beacons.",
            "The key points are: 1) Extracts both visual and textual relational information for construction safety inspection. 2) Integrates NLP and computer vision methods. 3) Enables automatic safety checking by matching textual and visual relations. The value is providing an automated way to identify hazards by understanding construction scenes and regulations.",
            "Proposes a context-aware safety monitoring system using computer vision models. Demonstrates distinguishing workers' context is important to reduce false alarms. Compares two approaches using depth estimation and extended segmentation labels. The modular system design allows replacing each module.",
            "The system's modularity allows easy reconfiguration and expansion of capabilities. Monitoring human activities in critical environments like pandemic outbreaks can help enforce safety rules.",
            "Proposes flexible 4-step procedure to automate compliance checking for diverse PPE types protecting different body parts. Shows improved accuracy over prior detection-based approaches. Demonstrates modularity and ability to easily adapt for new PPE types."
           ],
           [
            "Use more advanced domain adaptation techniques to better adapt models from synthetic to real images.",
            "Self-configuring methods for accurate distance estimation without pre-mapping. Real-time adaptation of beacon models.",
            "Expand the dataset size and types of hazards covered. Consider spatial relations and human pose estimation in scene graph generation. Build a construction knowledge graph for hazard prediction.",
            "Fine-tune depth estimation model with construction site data. Use advanced training techniques to improve small object detection. Integrate detection and segmentation into a single pipeline.",
            "Automatically select best counting approach based on scene. Expand with more capabilities like gesture recognition. Use synthetic data for social distance training.",
            "Address effects like occlusion and low image quality. Analyze video instead of single frames. Expand to more PPE varieties and industries."
           ],
           [
            "Many more variations in real images of some classes like heads and welding masks compared to synthetic images.",
            "Requires modeling phase for each BLE transmitter. Materials absorbing radio waves could distort estimates.",
            "Small dataset size. Limited types of relations detected in images.",
            "Depth estimation performance is not ideal without construction site data. Independent detection and segmentation pipelines require more computation. Small object detection performance is lower than large objects.",
            "Limited to monitoring single planar surfaces. Density estimation struggles in extremely crowded scenes.",
            "Privacy constraints and computational costs limit applicability for whole-facility 24/7 monitoring. Performance may vary for less common PPE types."
           ],
           [
            "Synthetic dataset of 126,900 images generated from Grand Theft Auto V game engine. Real-world test set of 180 images.",
            "Collected own dataset. Did not share dataset.",
            "Self-built datasets for Chinese safety regulations text and construction site images with annotations.",
            "YUD-COSAv2 collected from real construction sites. It contains 1089 images with annotations for object detection and semantic segmentation.",
            "New datasets collected: CrowdVisorPisa for pedestrian monitoring, and CrowdVisorPPE for PPE detection. Also uses various public datasets like MOT and ShanghaiTech.",
            "Combined web-mined images and public PPE datasets (Roboflow hardhat train data and Pictor PPE data). After cropping ROIs, datasets of 400-8460 images per PPE type."
           ],
           [
            "Yes, datasets made publicly available",
            "No",
            "Not specified",
            "Available upon request",
            "CrowdVisorPisa and CrowdVisorPPE provided upon request",
            "The Roboflow and Pictor datasets are publicly available. The full combined dataset used is not shared."
           ],
           [
            "Interesting approach for training deep learning models when labelled real data is scarce. Game engines seem to provide a viable synthetic data source.",
            "Interesting use of multiple low-cost BLE beacons for safety monitoring. Active risk mitigation is valuable. Approach seems flexible for many applications.",
            "The integration of NLP and computer vision is promising for automating compliance checking and safety inspections. More complex reasoning with knowledge graphs could enable a 'virtual safety inspector'.",
            "The idea of using explicit height labels in segmentation is interesting. The modular design allows customizing the system for different applications.",
            "The system seems flexible and practical for safety monitoring applications. Interesting use of both synthetic and real data. More work needed to handle complex environments.",
            "The flexibility to easily customize for different PPE types and industries is useful. Analyzing short video clips could improve reliability compared to single frames."
           ],
           [
            "Partially supported by CUP CIPE D55F17000290009 and AI4EU project funded by EC (H2020 - Contract n. 825619)",
            "Plan Propio-Universidad de Málaga",
            "Partially funded by Aerospace Hongka Intelligent Technology (Beijing) CO., LTD.",
            "National R&D Project for Smart Construction Technology, Korea Agency for Infrastructure Technology Advancement",
            "Partially supported by EU H2020 projects like AI4EU and AI4Media",
            "Funded by the Science Fund of the Republic of Serbia, project ID 6524219 - AI4WorkplaceSafety."
           ],
           [
            "1.json",
            "2.json",
            "3.json",
            "4.json",
            "5.json",
            "6.json"
           ]
          ]
         },
         "header": {
          "align": "left",
          "fill": {
           "color": "paleturquoise"
          },
          "values": [
           "Article name",
           "Citation",
           "Year of the article",
           "DOI",
           "Brief summary",
           "What is detected in this paper?",
           "Detected object hashtag",
           "Method and the technology employed in the paper for detection",
           "Primary focus",
           "Other technologies or tools used",
           "Method of measuring performance",
           "Achieved performance",
           "Key points and the value of the research",
           "Suggestions or recommendations for future research",
           "Limitations",
           "Dataset",
           "Sharing Dataset?",
           "Personal ideas or insights",
           "Funding",
           "Filename"
          ]
         },
         "type": "table"
        }
       ],
       "layout": {
        "height": 800,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 5000
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If you want to display more colorful table of results\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[go.Table(\n",
    "    header=dict(values=list(df.columns),\n",
    "                fill_color='paleturquoise',\n",
    "                align='left'),\n",
    "    cells=dict(values=[df[col] for col in df.columns],\n",
    "               fill_color='lavender',\n",
    "               align='left'))\n",
    "])\n",
    "\n",
    "fig.update_layout(width=5000, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nougat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
