Article name,Citation,Year of the article,DOI,Brief summary,What is detected in this paper?,Detected object hashtag,Method and the technology employed in the paper for detection,Primary focus,Other technologies or tools used,Method of measuring performance,Achieved performance,Key points and the value of the research,Suggestions or recommendations for future research,Limitations,Dataset,Sharing Dataset?,Personal ideas or insights,Funding,Filename
Learning accurate personal protective equipment detection from virtual worlds,"(Di Benedetto et al., 2021)",2021,10.1007/s11042-020-09597-9,The paper proposes using photo-realistic synthetic images generated from a game engine to train deep learning models for detecting personal protective equipment (PPE). The models are then adapted to real-world images using a small set of real images. The approach is shown to be effective for PPE detection applications where limited real training data is available.,"Personal protective equipment like helmets, welding masks, ear protection, high visibility vests","{'PPE': ['Helmet', 'Welding Mask', 'Ear Protection', 'High-Visibility Vest'], 'Actions': [], 'Body posture': []}",YOLO v3 and Faster R-CNN deep learning models trained on synthetic images from Grand Theft Auto V game engine and adapted to real images.,#vision,"['Grand Theft Auto V game engine', 'RAGE plugin']",Mean average precision (mAP),"76.1% mAP with YOLOv3, 77.1% mAP with Faster R-CNN after adaptation",Shows effectiveness of using synthetic training data from game engines for training deep learning models when real training data is limited. Adaptation to real images with small real dataset is effective.,Use more advanced domain adaptation techniques to better adapt models from synthetic to real images.,Many more variations in real images of some classes like heads and welding masks compared to synthetic images.,"Synthetic dataset of 126,900 images generated from Grand Theft Auto V game engine. Real-world test set of 180 images.","Yes, datasets made publicly available",Interesting approach for training deep learning models when labelled real data is scarce. Game engines seem to provide a viable synthetic data source.,Partially supported by CUP CIPE D55F17000290009 and AI4EU project funded by EC (H2020 - Contract n. 825619),1.json
A Safety System based on Bluetooth Low Energy (BLE) to prevent the misuse of Personal Protection Equipment (PPE) in construction,"(Gómez-de-Gabriel et al., 2022)",2022,https://doi.org/10.1016/j.ssci.2022.105995,"The paper proposes a system to monitor the use of personal protective equipment (PPE) like earmuffs in construction sites. It uses multiple Bluetooth Low Energy (BLE) beacons on the PPE and a receiver on the tool to estimate distance between worker and tool. If distance is unsafe or PPE is not worn properly, the system can turn off tool power to prevent injury.","Use of PPE (earmuff), Distance between worker and tool","{'PPE': 'Earmuff', 'Actions': '', 'Body posture': ''}","Multiple BLE beacons placed orthogonally on PPE, RSSI signals received by tool, Extended Kalman Filter and Bayesian filtering to estimate distance",#vision,"Microcontroller, Light sensor",Compared estimated distance to ground truth measurements,Expected error around 20 cm,Low cost system to monitor PPE use and worker-tool distance. Can actively turn off tool power instead of just warning. Robust to noise due to multiple orthogonal BLE beacons.,Self-configuring methods for accurate distance estimation without pre-mapping. Real-time adaptation of beacon models.,Requires modeling phase for each BLE transmitter. Materials absorbing radio waves could distort estimates.,Collected own dataset. Did not share dataset.,No,Interesting use of multiple low-cost BLE beacons for safety monitoring. Active risk mitigation is valuable. Approach seems flexible for many applications.,Plan Propio-Universidad de Málaga,2.json
Automatic Construction Hazard Identification Integrating On-Site Scene Graphs with Information Extraction in Outfield Test,"Liu, X., Jing, X., Zhu, Q., Du, W., & Wang, X. (2023). Automatic Construction Hazard Identification Integrating On-Site Scene Graphs with Information Extraction in Outfield Test. Buildings, 13(2), 377. https://doi.org/10.3390/buildings13020377",2023,https://doi.org/10.3390/buildings13020377,"The paper proposes a framework to automatically identify construction hazards by integrating on-site scene graph generation and BERT-based information extraction from safety regulations text. The information extraction model extracts key textual relational triples from regulations. The scene graph model detects visual relations between objects in images. By matching the textual and visual relational triples, the framework can conduct automatic safety checking and hazard identification.","The paper detects interactions between construction workers, personal protective equipment (PPE), and other objects in images of construction sites.","{'PPE': ['hard hats', 'eye protection', 'hand protection', 'face protection'], 'Actions': ['wearing', 'holding', 'standing', 'performing operations'], 'Body posture': []}",The paper uses a BERT-based model for information extraction from text and a CNN-based model for scene graph generation from images. The BERT model extracts key entities and relations from safety regulation text. The scene graph model uses Mask R-CNN for object detection and a separate CNN branch for relation detection.,#vision,"['fastText for word embeddings', 'LabelImg and PySimpleGUI for image annotation']","For information extraction: precision, recall, F1 score. For scene graph generation: average precision, recall@K.",79.3% F1 score for information extraction. 50.7% Recall@20 for scene graph generation.,The key points are: 1) Extracts both visual and textual relational information for construction safety inspection. 2) Integrates NLP and computer vision methods. 3) Enables automatic safety checking by matching textual and visual relations. The value is providing an automated way to identify hazards by understanding construction scenes and regulations.,Expand the dataset size and types of hazards covered. Consider spatial relations and human pose estimation in scene graph generation. Build a construction knowledge graph for hazard prediction.,Small dataset size. Limited types of relations detected in images.,Self-built datasets for Chinese safety regulations text and construction site images with annotations.,Not specified,The integration of NLP and computer vision is promising for automating compliance checking and safety inspections. More complex reasoning with knowledge graphs could enable a 'virtual safety inspector'.,"Partially funded by Aerospace Hongka Intelligent Technology (Beijing) CO., LTD.",3.json
Context-aware safety assessment system for far-field monitoring,"(Chern et al., 2023)",2023,https://doi.org/10.1016/j.autcon.2023.104779,"The paper proposes a context-aware safety monitoring system to assess workers' safety compliance in far-field construction site monitoring. It uses object detection, semantic segmentation and depth estimation models to distinguish workers at height vs on ground. This allows applying different PPE rules based on working context to reduce false alarms. Two approaches are tested using depth estimation and extended segmentation. The extended segmentation approach performed better in experiments.","Workers, PPE (hardhats, harnesses, straps, hooks), working context (height vs ground)","{'PPE': ['hardhats', 'harnesses', 'straps', 'hooks'], 'Actions': [], 'Body posture': []}","Object detection (YOLOv5), semantic segmentation (FPN), and depth estimation (pre-trained model by Alhashim and Wonka 2018) are used. The models are trained on the YUD-COSAv2 dataset collected from real construction sites.",#vision,"['OpenCV', 'PyTorch']","For object detection: mAP@0.5. For semantic segmentation: mIoU. For safety classification: Precision, Recall, F1-score.",mAP of 85.3% for YOLOv5. mIoU of 75.74% for FPN. F1-score of 95.07% for safety classification using extended segmentation.,Proposes a context-aware safety monitoring system using computer vision models. Demonstrates distinguishing workers' context is important to reduce false alarms. Compares two approaches using depth estimation and extended segmentation labels. The modular system design allows replacing each module.,Fine-tune depth estimation model with construction site data. Use advanced training techniques to improve small object detection. Integrate detection and segmentation into a single pipeline.,Depth estimation performance is not ideal without construction site data. Independent detection and segmentation pipelines require more computation. Small object detection performance is lower than large objects.,YUD-COSAv2 collected from real construction sites. It contains 1089 images with annotations for object detection and semantic segmentation.,Available upon request,The idea of using explicit height labels in segmentation is interesting. The modular design allows customizing the system for different applications.,"National R&D Project for Smart Construction Technology, Korea Agency for Infrastructure Technology Advancement",4.json
An embedded toolset for human activity monitoring in critical environments,"(Di Benedetto et al., 2022)",2022,https://doi.org/10.1016/j.eswa.2022.117125,"The paper presents a modular embedded system for monitoring human activities in critical environments. The system uses computer vision and AI to perform tasks like estimating crowd density, measuring social distance, and detecting personal protective equipment. The modules can be combined and configured based on needs. Experiments validate the system's effectiveness for safety monitoring.","The system detects pedestrians, measures interpersonal distances, estimates crowd density, and detects personal protective equipment like helmets, high-visibility vests, and face masks.","{'PPE': ['Helmets', 'High-visibility vests', 'Face masks'], 'Actions': [], 'Body posture': []}",The system uses deep learning models like Faster R-CNN for pedestrian and PPE detection. It also uses a density estimation model based on CSRNet for crowd density estimation.,#vision,"['Faster R-CNN', 'CSRNet', 'DeepSort tracker']","Mean average precision (mAP) for detection tasks. Mean absolute error (MAE), root mean squared error (RMSE) and structural similarity index (SSIM) for density estimation.","Pedestrian detection mAP: 0.836. PPE detection mAP: 0.606. Density estimation MAE: 92.28, RMSE: 365.4, SSIM: 0.79.",The system's modularity allows easy reconfiguration and expansion of capabilities. Monitoring human activities in critical environments like pandemic outbreaks can help enforce safety rules.,Automatically select best counting approach based on scene. Expand with more capabilities like gesture recognition. Use synthetic data for social distance training.,Limited to monitoring single planar surfaces. Density estimation struggles in extremely crowded scenes.,"New datasets collected: CrowdVisorPisa for pedestrian monitoring, and CrowdVisorPPE for PPE detection. Also uses various public datasets like MOT and ShanghaiTech.",CrowdVisorPisa and CrowdVisorPPE provided upon request,The system seems flexible and practical for safety monitoring applications. Interesting use of both synthetic and real data. More work needed to handle complex environments.,Partially supported by EU H2020 projects like AI4EU and AI4Media,5.json
Generic compliance of industrial PPE by using deep learning techniques,"(Vukicevic et al., 2022)",2022,https://doi.org/10.1016/j.ssci.2021.105646,"The paper proposes a 4-step procedure to automate compliance of personal protective equipment (PPE) in industrial settings using deep learning. It detects workers using pose estimation, defines regions of interest (ROI) for different PPE types, and classifies the ROIs as compliant or not. The approach is evaluated on 18 PPE types protecting different body parts, achieving 95% accuracy with MobileNetV2. It demonstrates improved performance and flexibility compared to prior work.",Compliance of 18 different types of personal protective equipment (PPE).,"{'PPE': ['Face mask', 'Respiratory mask', 'Earmuffs', 'Welding mask', 'Face shields', 'Safety glasses', 'Hardhat', 'Head cover', 'Yellow vests', 'Visibility tracks', 'Exam gloves', 'Industry gloves', 'Sandals', 'Industry shoes', 'Boots', 'Feet covers', 'Protective cloth'], 'Body parts': ['Respiratory system', 'Hearing system', 'Face', 'Eyes', 'Head', 'Upper body', 'Hands', 'Feet', 'Whole body']}","Uses HigherHRNet pose estimator to detect workers and define regions of interest (ROI) for different PPE types. Classifies each ROI with deep learning classifiers (MobileNetV2, DenseNet, ResNet etc.) to determine PPE compliance.",#vision,"['HigherHRNet pose estimator', 'MobileNetV2', 'DenseNet', 'ResNet', 'PyTorch']","Accuracy, Precision, Recall, F1 Score",95% overall accuracy with MobileNetV2,Proposes flexible 4-step procedure to automate compliance checking for diverse PPE types protecting different body parts. Shows improved accuracy over prior detection-based approaches. Demonstrates modularity and ability to easily adapt for new PPE types.,Address effects like occlusion and low image quality. Analyze video instead of single frames. Expand to more PPE varieties and industries.,Privacy constraints and computational costs limit applicability for whole-facility 24/7 monitoring. Performance may vary for less common PPE types.,"Combined web-mined images and public PPE datasets (Roboflow hardhat train data and Pictor PPE data). After cropping ROIs, datasets of 400-8460 images per PPE type.",The Roboflow and Pictor datasets are publicly available. The full combined dataset used is not shared.,The flexibility to easily customize for different PPE types and industries is useful. Analyzing short video clips could improve reliability compared to single frames.,"Funded by the Science Fund of the Republic of Serbia, project ID 6524219 - AI4WorkplaceSafety.",6.json
